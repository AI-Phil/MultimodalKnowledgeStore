{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a virtual environment or similar (this was built with Python 3.10, but 3.11 should work too), and install `requirements.txt`:\n",
    "    ```bash\n",
    "    pip install -r requirements.txt\n",
    "    ```\n",
    "2. Setup Google Cloud Application Default Credentials (see [this doc](https://cloud.google.com/docs/authentication/provide-credentials-adc)).\n",
    "3. Copy the `.env.template` file and set keys and other information as indicated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM and Data Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the `.env` file into the Python environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize VertexAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import vertexai\n",
    "vertexai.init(\n",
    "    project=os.environ.get(\"GOOGLE_PROJECT_NAME\"),\n",
    "    location=os.environ.get(\"GOOGLE_LOCATION\",'us-east1'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize LangChain VertexAI components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import VertexAI\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "\n",
    "llmModel = VertexAI(model_name=os.environ.get('GOOGLE_LLM','gemini-1.5-flash'))\n",
    "chatModel = ChatVertexAI(model=os.environ.get('GOOGLE_LLM','gemini-1.5-flash'))\n",
    "embedModel = VertexAIEmbeddings(model_name=os.environ.get('GOOGLE_EMBED_MODEL','multimodalembedding')) \n",
    "genModel = GenerativeModel(model_name=os.environ.get('GOOGLE_LLM','gemini-1.5-flash'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Cassio (Astra DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cassio\n",
    "cassio.init(auto=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And establish the graph store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragstack_langchain.graph_store import CassandraGraphStore     \n",
    "\n",
    "SITE_PREFIX=\"travel_docs\"\n",
    "graph_store = CassandraGraphStore(\n",
    "    embedModel,\n",
    "    node_table=f\"{SITE_PREFIX}_nodes\",\n",
    "    edge_table=f\"{SITE_PREFIX}_edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create LangChain `Document`s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example `Tourbook.pdf` is fairly complex in structure, both digitally and visually. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variety of parsing tools such as Unstructured and Adobe ExtractAPI were attempted on `Tourbook.pdf` file, attempting with both file structure and OCR techniques, to no avail. The Vertex LLM was able to parse (with a fairly generic prompt), but unfortunately exited early as it determined it was repeating existing content and cited the URL of this!\n",
    "\n",
    "In this notebook we are trying to demonstrate multi-modal embedding and retrieval, so the information was manually parsed, and put into the file `Tourbook.json`. This contains the first 24 pages minus the cover page, the table of contents, and a map on page 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from ragstack_knowledge_store.link_tag import BidirLinkTag\n",
    "import json\n",
    "\n",
    "with open('Tourbook.json', 'r') as file:\n",
    "    text_data = json.load(file)\n",
    "\n",
    "text_documents = []\n",
    "h1_dict = {}\n",
    "\n",
    "for i, entry in enumerate(text_data):\n",
    "    h1 = entry['metadata']['h1']\n",
    "    h1_dict[entry['metadata']['page_number']] = h1 # note the H1 level for each page, as we will reference again on the images\n",
    "    link_h1 = BidirLinkTag(kind=\"h1\", tag=h1)\n",
    "    entry['metadata']['link_tags'] = [link_h1]\n",
    "    doc = Document(page_content=entry['page_content'], metadata=entry['metadata'])\n",
    "    text_documents.append(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the `metadata.link_tags` list; here we are linking to and from the H1 header level, which corresponds to the section. In this way, any information in a section will be linked to other information in the section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For images, we will use `PyMuPDF` to extract images from the document, `base64` encode the image, and create a `Document` referencing the appropriate H1 heading for the page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf\n",
    "import base64\n",
    "\n",
    "doc = pymupdf.open('Tourbook.pdf')\n",
    "image_documents = []\n",
    "\n",
    "# page_index starts from 0, so these are actual pages 3, 5-23, but are numbered 1 and 3-21. \n",
    "pages_to_process = [2] + list(range(4, 23))  \n",
    "\n",
    "for page_index in pages_to_process:\n",
    "    page = doc[page_index]\n",
    "    image_list = page.get_images()\n",
    "    adjusted_page_number = page_index - 1\n",
    "\n",
    "    # Iterate over the images on the page\n",
    "    for image_index, img in enumerate(image_list, start=1):\n",
    "        xref = img[0]\n",
    "        pix = pymupdf.Pixmap(doc, xref) \n",
    "        if pix.n - pix.alpha > 3:\n",
    "            pix = pymupdf.Pixmap(pymupdf.csRGB, pix)\n",
    "\n",
    "        base64_image = base64.b64encode(pix.tobytes(output=\"png\")).decode('utf-8')\n",
    "\n",
    "        if adjusted_page_number % 2 == 0:  # If it's even\n",
    "            page_spread = f\"{adjusted_page_number}-{adjusted_page_number+1}\"\n",
    "        else:  # If it's odd\n",
    "            page_spread = f\"{adjusted_page_number-1}-{adjusted_page_number}\"\n",
    "\n",
    "        h1 = h1_dict[adjusted_page_number]\n",
    "        link_h1 = BidirLinkTag(kind=\"h1\", tag=h1)\n",
    "        doc_metadata = {\n",
    "            \"mime_type\": \"image/png\",\n",
    "            \"mime_encoding\": \"base64\",\n",
    "            \"page_number\": adjusted_page_number, \n",
    "            \"page_spread\": page_spread, \n",
    "            \"image_index\": image_index,\n",
    "            \"h1\": h1, \n",
    "            \"link_tags\" : [ link_h1 ]\n",
    "\n",
    "        }\n",
    "        # Now in theory, langchain_google_vertex.embeddings.embed_image() calls ImageBytesLoader.load_bytes\n",
    "        # which can take a base64 string, but that wasn't working...but this URI trick does work!\n",
    "        image_doc = Document(page_content=f\"data:image/png;base64,{base64_image}\", metadata=doc_metadata)\n",
    "        image_documents.append(image_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Knowledge Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for doc in text_documents + image_documents:\n",
    "    docs.append(doc)\n",
    "\n",
    "    if len(docs) >= 50:\n",
    "        print(\"saving batch\")\n",
    "        graph_store.add_documents(docs)\n",
    "        docs.clear()\n",
    "\n",
    "if docs:\n",
    "    print(\"saving batch\")\n",
    "    graph_store.add_documents(docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
